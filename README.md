# gRPC-Kafka-Threejs

## General Objective
This project has general objective to be able to show a moving object in ThreeJS controlled by a completely different service.

## Technologies used
- Python 3
- gRPC
- Kafka
- NodeJS + ThreeJS

## Specific Objectives
- Create a common payload using gRPC that can be used in python and javascript
- Data Broker: Have a kafka broker running to interchange data between services
- Data Generator: Python program that generates random numbers and sends them to the broker
- Data Validator: Python program that connects to the broker, reads, and prints the numbers generated by the generator
- Data Reader: JS (node) program that reads the kafka messages
- HTML Server: Frontend dispatcher with the threeJS graphics
- Web Sockets Server: Allow communication between the data reader and the frontend

## Project Description
Generate random numbers in a specified range of desired values that will be sent to a kafka broker (to reduce services interdependency\*). This numbers will be read and sent to the frontend through a web sockets server. In the browser a sphere will be shown and it will move accross the x-axis according to the  random number generated by the random number generator.

\* Meaning that we can turn one service on/off without worrying about the other services. Since the project has unidirectional data flow, it is totally duable.

## Components Description

### Payloads
The payloads are the messages interchanged across services. I use gRPC as it offers a wide variety of supported languages. Since my interest is only to move an object through the x-axis, I just need a message with a float value. The payload I use is described in [this](CHANGE_ME) file.

### Data Broker
You may wondering why use a broker when using gRPC we can already communicate with other services. The answer to this is that doing so would require that all the services existed at all time. With a (kafka) broker in the middle of the communication it gives me the freedom of shutting down/up almost any service at any given time without affecting the others. Note that this works because I am not doing bidirectional communication with request from one service to the other. I am simply sending data to the broker and broadcasting it to whoever wants it by connecting to the broker.

Another thing to take in mind with kafka is that is a two-parts software; it need the zookeeper (handles multiple brokers) to exists in order for the broker(s) to be functional. You can find more information on internet.

### Data Generator
This is a Python sript that simply generates data in a range of values and sends them to the broker. The data is first added to a payload object and then serialized before being sent.

### Data Validator
This is just a Python utility to make sure the data is correctly being sent to the broker. It simply connects to the broker, deserializes the retrieved data into a payload object, and prints its value to the terminal. Since I am unexperienced with JS, this is a way to objerve that data is flowing and that if something is happening in the JS side, a way to discard the data generation section of the pipeline.

### Data Reader
Reads the kafka broker and deserializes the data into a payload. Then, the data is transmited to the web sockets server so it can be used in the frontend. This is written in JS and run by node.

### HTML Server
It is in charge of delivering the HTML/JS (frontend) to any browser accessing its connection URL. Note that the JS script delivered to the browser, apart from being the ThreeJS script, it also contains a web sockets client connecting th our web sockets server. This is the way our payload reaches the browser, allowing the ThreeJS object to move

### Web Sockets Server
The browser most probably could have reached the kafka brocker through its REST API, but I simply wanted to do it this way. Plus, it may not be always the case that the broker is reachable (thinking in production right now). It may be the case that there are more types of data flowing through it and you don't want anyone or anything but your server to be able to reach it.


## Requirements

### gRPC
For the python protos do `pip install grpcio-tools`. For the JS protos, download the binares of version 3.14.0 (`protoc-3.14.0-linux-x86_64.zip`) from [here](https://github.com/protocolbuffers/protobuf/releases) and unzip it in `00_payload/grpc`. The `protoc` bin should be added to the path. Though, in the script for generating the payloads I already add it temporarily. Inside `00_payload` there's the commands used to generate the payloads in `generate_protos.sh`.

### Kafka
For the kafka broker we will use the binaries of the 2.7.0 version. I downloaded them from [here](https://github.com/protocolbuffers/protobuf/releases) and place the file content under `01_broker/kafka_2.13-2.7.0`

```bash
# Download the file using wget
$ wget https://www.apache.org/dyn/closer.cgi?path=/kafka/2.7.0/kafka_2.13-2.7.0.tgz

# Decompress the file
$ tar -zxvf kafka_2.13-2.7.0.tgz
```

Note that I changed the `kafka_2.13-2.7.0/config/server.properties` a bit. The options, as I changed them, are shown below.

```
....

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=1

....

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

....
```

### NodeJS
This will be dependent on the system being used. Look at the internet for instructions. For required dependencies do as below. Though, the package-lock should contain the dependences I had in case I am missing something.

```
$ npm install node-rdkafka ws grpc-web
```

## How to execute
This is more involved than I would like to, but sice this is a learning exercise it is okay. I recommend using Tmux for having multiple panes/windows/tabs/sessions.

### Kafka

Run this in one terminal
```
$ cd 01_broker
$ ./run_kafka_zookeeper.sh
```

And this in another
```
$ cd 01_broker
$ ./run_kafka_zookeeper.sh
```

### Data Generator

```
$ cd 00_payload
$ python data_generator.py
```

### Data Validation

```
$ cd 00_payload
$ python read_data.py
```

### NodeJs section

```
$ cd 03_nodejs/scripts
$ node server.js
```
```
$ cd 03_nodejs/scripts
$ node ws-server.js
```
```
$ cd 03_nodejs/scripts
$ node kafka-consumer.js
```
